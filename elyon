#!/usr/bin/python3
__author__ = 'daniel'
import getopt, datetime, sys, os
import traceback
import pickle
import crawler_methods
from elyon_crawler import ElyonCrawler


def serialize(lst):
    if len(lst) > 0:
        with open('elyon.fails', 'wb') as f:
            pickle.dump(lst, f)
            f.flush()


def deserialize():
    with open('elyon.fails', 'rb') as f:
        s = pickle.load(f)
    return s


def main(argv):
    SCRIPT_HELP = '''ElyonCrawler v1.0 by Daniel Mittelman, 2015
Usage: elyon [Options] [Targets] [-b dd/mm/yyyy] [-e dd/mm/yyyy]
A web crawler that extracts detailed verdict information from the Israeli Supreme Court's online data sources

Crawl work mode:
    -b                        Start crawling from this date
    -e                        Stop crawling when reaching this date
    -R, --retry               Retry failed verdicts from a previous run. Overrides the -b and -e flags

Output targets:
    -o, --output              Output SQLite database file path (Default: elyon.db)
    -l, --log                 Output log file path (Default: elyon.log)

Options:
    -T, --include-technical   Include technical resolutions
    -F, --full-text           Save the full text of the document (Warning: increases output size significantly)
    -S, --skip-confidential   Do not write confidential verdicts to the output database
    -t, --timeout             Connection timeout in seconds (Default: 30)
    -c, --threads             Number of crawl threads, between 1 and 10 (Default: 5)
    -q                        Be quiet, write only critical messages to the standard output
    -y                        Assume "yes" to all questions
    -v, --verbose             Be verbose
    -h, --help                Show this help prompt

Examples:
    Download all verdicts published between 15/02/2015 and 02/03/2015:
        elyon -b 15/02/2015 -e 02/03/2015
    Download all verdicts published between 15/02/2015 and today into output.db, using 8 threads:
        elyon -o output.db -b 15/02/2015 -c 8
    Download all verdicts (since 1987) except for confidential verdicts, using a timeout of 90 seconds:
        elyon -S -t 90
    Complete the last run by downloading all the verdicts that have failed
        elyon -R
'''
    start, end = datetime.datetime(1987, 11, 18), datetime.datetime.today()
    retry = False
    logfile, outputfile = 'elyon.log', 'elyon.db'
    verbose = False
    quiet = False
    skip_confidential = False
    timeout = 30
    threads = 5
    technical = False
    auto_yes = False
    full_text = False

    if len(argv) == 0:
        print(SCRIPT_HELP)
        sys.exit(0)

    try:
        opts, args = getopt.getopt(argv, "b:e:o:l:t:c:STRFqvhdy",
                                   ["help", "verbose", "output", "log", "skip-confidential",
                                    "timeout", "threads", "include-technical", "full-text", "retry"])
    except getopt.GetoptError:
        print(SCRIPT_HELP)
        sys.exit(-1)

    for opt, arg in opts:
        if opt in ("-h", "--help"):
            print(SCRIPT_HELP)
            sys.exit(0)
        elif opt == "-b":
            start = datetime.datetime.strptime(arg, "%d/%m/%Y")
        elif opt == "-e":
            end = datetime.datetime.strptime(arg, "%d/%m/%Y")
        elif opt in ("-R", "--retry"):
            retry = True
        elif opt in ("-o", "--output"):
            outputfile = arg
        elif opt in ("-l", "--log"):
            logfile = arg
        elif opt in ("-S", "--skip-confidential"):
            skip_confidential = True
        elif opt in ("-T", "--include-technical"):
            technical = True
        elif opt in ("-F", "--full-text"):
            full_text = True
        elif opt == "-q":
            quiet = True
        elif opt in ("-t", "--timeout"):
            timeout = int(arg)
        elif opt in ("-c", "--threads"):
            if int(arg) > 10:
                print("Cannot spawn more than 10 concurrent crawl threads!")
                sys.exit(-1)
            threads = int(arg)
        elif opt == "-y":
            auto_yes = True
        elif opt in ("-v", "--verbose"):
            verbose = True

    # If crawling over a period of more than 30 days, show a warning prompt
    if not auto_yes and (end - start).days > 30:
        print("This will extract over 30 days of verdicts! Are you sure? (y/N): ", end='', flush=True)
        if sys.stdin.read(1) != "y":
            return

    c = ElyonCrawler(start, end, logfile, outputfile, verbose, quiet,
                     skip_confidential, timeout, threads, technical, full_text)

    # Start the crawling process. Upon a critical error or a keyboard interrupt,
    # serialize the list of failures to file and exit with the appropriate exit code
    try:
        if not retry:
            c.crawl()
        else:
            c.retry_crawl(deserialize())
    except KeyboardInterrupt:
        c.log_message(crawler_methods.LogLevel.INFO,
                      "Exiting on user request")
    except Exception as e:
        c.log_message(crawler_methods.LogLevel.ERROR,
                                "Crawler exception: " + str(e) + os.linesep + traceback.format_exc())
    finally:
        # Serialize all the FaultEntity instances into file and exit
        serialize(c.get_retry_list())
        sys.exit()

if __name__ == "__main__":
    main(sys.argv[1:])